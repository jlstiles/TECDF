\documentclass[11pt]{article}
\usepackage[authoryear,round]{natbib}
\usepackage{graphicx}
%\usepackage{psfig}
%deleted the following hyperlink command since bepress cannot handle this
%\usepackage{hyperref}
\usepackage{latexsym}
\usepackage[left=1.5in,top=1.5in,right=1.5in,bottom=1.5in]{geometry}\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usetikzlibrary{arrows}
\definecolor{darkblue}{rgb}{0,0.4,0.9}
\definecolor{gray10}{rgb}{0.1,0.1,0.1}
\definecolor{gray20}{rgb}{0.2,0.2,0.2}
\definecolor{gray30}{rgb}{0.3,0.3,0.3}
\definecolor{gray40}{rgb}{0.4,0.4,0.4}
\definecolor{gray60}{rgb}{0.6,0.6,0.6}
\definecolor{gray80}{rgb}{0.8,0.8,0.8}
\definecolor{gray90}{rgb}{0.9,0.9,.9}
\definecolor{gray95}{rgb}{0.95,0.95,.95}
\definecolor{gray96}{rgb}{0.96,0.96,.96}
\definecolor{lgreen} {RGB}{180,210,100}
\definecolor{dblue}  {RGB}{20,66,129}
\definecolor{ddblue} {RGB}{11,36,69}
\definecolor{lred}   {RGB}{220,0,0}
\definecolor{nred}   {RGB}{224,0,0}
\definecolor{norange}{RGB}{230,120,20}
\definecolor{nyellow}{RGB}{255,221,0}
\definecolor{ngreen} {RGB}{98,158,31}
\definecolor{dgreen} {RGB}{78,138,21}
\definecolor{nblue}  {RGB}{28,130,185}
\definecolor{jblue}  {RGB}{20,50,100}
\definecolor{nnyellow}{RGB}{235,200,0}
\definecolor{purple}{RGB}{150, 0, 120}
%\definecolor{sgGreen} {RGB}{77, 175, 74} % good one
\definecolor{sgGreen} {RGB}{20, 180, 50}
\definecolor{revised}{rgb}{0,0,0.9}
\newtheorem{definition}{Definition}
\newtheorem{result}{Result}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newcommand{\nl}{\newline}
\newcommand{\pl}{\parallel}
\newcommand{\openr}{\hbox{${\rm I\kern-.2em R}$}}
\newcommand{\openn}{\hbox{${\rm I\kern-.2em N}$}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}\;} 
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\;} 





\bibliographystyle{plainnat}

%\pagestyle{empty}

\title{TMLE of Primitive of Cumulative Distribution of Conditional Treatment Effect}

%Susan: add yourself
\author{Mark van der Laan \\ Division of Biostatistics, University of California, Berkeley\\ {\tt laan@berkeley.edu}
\\
} \date{\today}
 %Susan : add affiliation
 \begin{document}
\maketitle
 
\begin{abstract}
 
\end{abstract}

{\bf Keywords}: 

\section{Introduction}
Let $O=(W,A,Y)\sim P_0$ and let the model ${\cal M}$ be nonparametric. Let $B(P)(W)=E_P(Y\mid A=1,W)-E_P(Y\mid A=0,W)$ and let 
$\Psi_y(P)=\int_{-1}^y E_PI(B(P)(W)>x) dx=\int_{-1}^y  S_B(x) dx$ be the target parameter of interest, where $S_B(x)=P(B(W)>x)$.
We note that we can also write this target parameter as  follows:
\[
\Psi_y(P)=E_P( \min(B(P)(W),y)+1)=1+E_P\min(B(P)(W),y).\]
%=E_P I(B(P)(W)\leq y)B(P)(W)+yE_PI(B(P)>y).\]
The latter representation demonstrates that this parameter is smoother in $P$ than $S_B(x)=E_PI(B(P)(W)>x)$: instead of having $B$ in an indicator it is now truncated from above (i.e. from non differentiable to continuous with a point of non differentiability). 
We will prove that $\Psi_y$ is pathwise differentiable at $P$ with   efficient influence curve given by:
\[
D^*_y(P)=\frac{2A-1}{g(A|W)}I(B(P)(W)\leq y)(Y-\bar{Q}(A,W))+\int_{-1}^yI(B(P)(W)>x)-\Psi_y(P).\]
Subsequently, we will study the remainder $P_0D^*_y(P)+\Psi_y(P)-\Psi_y(P_0)$.

\subsection{Pathwise differentiability of $\Psi_y$}
We first note that $b\rightarrow I(b>x)$ is a function in $b$ that jumps from 0 to 1 at $x$. One can think of this as a cumulative distribution function of a pointmass $1$ at $x$.
That is, we can define $F_{0,x}(b)=I(b>x)=\int_{-1}^b dF_{0,x}(u)$, where $dF_{0,x}$ is the degenerate probability distribution/density that puts mass $1$ at $x$.
Thus $\Psi_y(P)=\int_{-1}^y E_P F_{0,x}(B(P)(W)) dx$.
This suggest approximating $F_{0,x}$ with $F_{h,x}(b)=\int_{-1}^b K((u-x)/h)/h du$, where $K$ is a mean zero density with support $[-1,1]$.
Note that $F_{h,x}(b)$ approximates the step function $F_{0,x}(b)$ by smoothing its jump at $x$ from 0 to 1 over a little interval $[x-h,x+h]$. 
This suggest defining an approximate parameter \[
\Psi_{y,h}(P)=\int_{-1}^y E_P F_{h,x}(B(P)(W)) dx.
\]
Let's consider the function $\Psi_{y,h}(B)=\int_{-1}^y E_PF_{h,x}(B(W)) dx$, treating $P$ as known in the expectation $E_P$, and let's aim to understand if this parameter is pathwise differentiable and its behavior at $h\rightarrow 0$.
We have 
\begin{eqnarray*}
\Psi_{y,h}(B_{\epsilon})-\Psi_{y,h}(B)&=&\int_{-1}^y E_P \{F_{h,x}(B_{\epsilon}(W))-F_{h,x}(B(W))\} dx\\
&=&\int_{-1}^y E_P \frac{d}{dB(W)}F_{h,x}(B(W))(B_{\epsilon}(W)-B(W))dx\\
&&+ \int_{-1}^y E_P \frac{d^2}{db(W)}F_{h,x}(b(W))(B_{\epsilon}(W)-B(W))^2/2,\end{eqnarray*}
for a $b=b(x,h,W,\epsilon)$ in the interval spanned by the values $B_{\epsilon}(W)$ and $B(W)$.
Let's denote this last second order term with $R_{2,h,y}(B_{\epsilon},B)$.
We have \[
\frac{d}{db}F_{h,x}(b)=\frac{d}{db}\int_{-1}^b K((u-x)/h)/h du=K((b-x)/h)/h .\]
Thus, we obtain
\begin{eqnarray*}
\Psi_{y,h}(B_{\epsilon})-\Psi_{y,h}(B)&=&E_P \int_{-1}^y K((B(W)-x)/h)/h dx (B_{\epsilon}(W)-B(W))\\
&&+ R_{2,h,y}(B_{\epsilon},B)\\
&=&E_P F_{h,B(W)}(y) (B_{\epsilon}(W)-B(W))+R_{2,h,y}(B_{\epsilon},B)\\
&=&E_P(F_{0,B(W)}(y)(B_{\epsilon}(W)-B(W))+R_{2,h,y}(B_{\epsilon},B)\\
&&+ E_P(F_{h,B(W)}(y)-F_{0,B(W)}(y))(B_{\epsilon}(W)-B(W)).
\end{eqnarray*}
Note $F_{0,B(W)}(y)=I(B(W)\leq y)$.
As $h\rightarrow 0$, we have that $\pl F_{h,B(W)}(y)-F_{0,B(W)}(y)\pl_{P}=O(h)$ converges to zero.
Using Cauchy-Schwarz inequality, the last term is bounded by $\pl F_{h,B(W)}(y)-F_{0,B(W)}(y)\pl_P\pl B_{\epsilon}-B\pl_P$, which is thus
$O(h \pl B_{\epsilon}-B\pl_P)$.
We also note that the second derivative of $F_{h,x}$ in $R_{2,h,y}()$ behaves as $1/h$ so that \[
R_{2,h,y}(B_{\epsilon},B)=O(\pl B_{\epsilon}-B\pl^2/h).\]
Thus, 
\[
\Psi_{y,h}(B_{\epsilon})-\Psi_{y,h}(B)=E_P I(B(W)\leq y)(B_{\epsilon}(W)-B(W))+O(h\pl B_{\epsilon}-B\pl_P)+O(\pl B_{\epsilon}-B\pl^2_P/h).\]
Note $B_{\epsilon}-B=O(\epsilon)$.
Given a path $\{P_{\epsilon}:\epsilon\}$, we can select $h=h(\epsilon)\rightarrow 0$ so that $\epsilon/h\rightarrow 0$ so that
\[
\frac{\Psi_{y,h_{\epsilon}}(B(P_{\epsilon}))-\Psi_{y,h_{\epsilon}}(B(P))}{\epsilon}=
E_P I(B(W)\leq y)(B_{\epsilon}(W)-B(W))/\epsilon +o(1).\]
The right-hand side will converge to 
\[
P D^*_{y,1}(P) S,\]
where $S$ is the score of $\{P_{\epsilon}:\epsilon\}$ and 
\[
D^*_{y,1}(P)(O)=\frac{2A-1}{g(A\mid W)}I(B(W)\leq y)(Y-\bar{Q}(A,W)).\]
In this way, we have proven that for such a sequence $h=h(\epsilon)$
\[
\lim_{\epsilon\rightarrow 0}\frac{\Psi_{h_{\epsilon}}(P_{\epsilon})-\Psi_{h_{\epsilon}}(P)}{\epsilon}=
P D^*_y(P) S,\]
where
\[
D^*_y(P)=D^*_{y,1}(P)(O)+D^*_{y,2}(P)(O),\]
and \[
D^*_{y,2}(P)(O)=\int_{-1}^y I(B(P)(W)>x) dx-\Psi_y(P).\]
So we have the following theorem:
\begin{theorem}
Consider the target parameter $\Psi_{y,h}:{\cal M}\rightarrow\openr$ defined by
\[
\Psi_{y,h}(P)=\int_{-1}^y E_P F_{h,x}(B(P)(W)) dx.
\]
We have that for a path $\{P_{\epsilon}:\epsilon\}$ with score $S$ at $\epsilon =0$
\[
\frac{\Psi_{y,h}(P_{\epsilon})-\Psi_{y,h}(P)}{\epsilon}=P D^*_{y,h}(P)S+O(\epsilon/h),\]
where 
\[
D^*_{y,h}(P)=\frac{2A-1}{g(A\mid W)}F_{h,B(W)}(y)(Y-\bar{Q}(A,W))+\int_{-1}^y F_{h,x}(B(P)(W)) dx -\Psi_{y,h}(P).\]
For $h\rightarrow 0$ and $\epsilon/h\rightarrow 0$, we have
\[
\lim_{\epsilon\rightarrow 0}\frac{\Psi_{y,h_{\epsilon}}(P_{\epsilon})-\Psi_{y,h_{\epsilon}}(P)}{\epsilon}=PD^*_y(P)S.\]
\end{theorem}

In order to show that this result implies pathwise differentiability of $\Psi_{y,0}$, we would need to show the following lemma.
\begin{lemma}
\[
\begin{array}{l}
\displaystyle
\frac{\Psi_{y,h_{\epsilon}}(P_{\epsilon})-\Psi_{y,0}(P_{\epsilon})-\left\{\Psi_{y,h_{\epsilon}}(P)-\Psi_{y,0}(P)\right\}}{\epsilon}
\\
\displaystyle
= \frac{\int_{-1}^y P(F_{h,x}-F_{0,x})(B(P_{\epsilon})dx-\int_{-1}^y P(F_{h,x}-F_{0,x})(B(P)) dx}{\epsilon}\\
\displaystyle  \rightarrow 0,
\end{array}
\]
as $\epsilon\rightarrow 0$, $h_{\epsilon}\rightarrow 0$, $\epsilon/h_{\epsilon}\rightarrow 0$.
\end{lemma}
We have to check if the latter is true, but I suspect it is since we know that $(\Psi_{y,h_{\epsilon}}(P_{\epsilon})-\Psi_{y,h_{\epsilon}}(P))/\epsilon$ converges nicely (no singularity), and $(F_{h,x}-F_{0,x})(B(W))$ converges to zero in $L^2(P)$-norm, which should all that is relevant since we are averaging over distribution of $W$.
In that case, we have the following theorem.
\begin{theorem}
Recall $F_{0,x}(b)=I(b> x)$.
Consider the target parameter $\Psi_{y}:{\cal M}\rightarrow\openr$ defined by
\[
\Psi_{y}(P)=\int_{-1}^y E_P F_{0,x}(B(P)(W)) dx.
\]
We have that for a path $\{P_{\epsilon}:\epsilon\}$ with score $S$ at $\epsilon =0$
\[
\lim_{\epsilon\rightarrow 0}\frac{\Psi_{y}(P_{\epsilon})-\Psi_{y}(P)}{\epsilon}=P D^*_{y}(P)S,\]
where 
\[
D^*_{y}(P)=\frac{2A-1}{g(A\mid W)}F_{0,B(W)}(y)(Y-\bar{Q}(A,W))+\int_{-1}^y F_{0,x}(B(P)(W)) dx -\Psi_{y}(P).\]
\end{theorem}

\subsection{Study of Remainder}
Let's suppress $y$ in the notation for now. Let's study the remainder 
\[
R_{20}(P,P_0)=P_0D^*(P)+\Psi(P)-\Psi(P_0).\]
Let $Q_W(P)=Q_W(P_0)$, since the contribution from $Q_W(P),Q_W(P_0)$ is trivial. We have
\begin{eqnarray*}
P_0D^*(P)+\Psi(P)-\Psi(P_0)&=&E_{P_0}
\frac{2A-1}{g(A\mid W)}I(B(W)<y)(Y-\bar{Q}(A,W))\\
&& +\int_{-1}^y I(B(W)>x)dx-\int_{-1}^yP_0I(B>x) dx\\
&&+
\int_{-1}^y P_0I(B>x) dx-\int_{-1}^y P_0I(B_0>x)dx\\
&=& 
E_{P_0}
\frac{2A-1}{g(A\mid W)}I(B(W)<y)(Y-\bar{Q}(A,W))\\
&& +\int_{-1}^y I(B(W)>x)dx-\int_{-1}^y P_0I(B_0>x)dx\\
&=& E_{P_0} \frac{g_0(1\mid W)}{g(1\mid W)}I(B(W)<y)(\bar{Q}_0-\bar{Q})(1,W)\\
&&-E_{P_0}\frac{g_0(0\mid W)}{g(0\mid W)}I(B(W)<y)(\bar{Q}_0-\bar{Q})(0,W)\\
&&+\int_{-1}^y I(B(W)>x)dx-\int_{-1}^y P_0I(B_0>x)dx\\
&=&E_{P_0}I(B(W)<y)(B_0-B)(W)+\int_{-1}^y P_0I(B>x)dx\\
&&-\int_{-1}^y P_0I(B_0>x)dx+R_{20,1}(g,g_0,\bar{Q},\bar{Q}_0),
\end{eqnarray*}
where
\begin{eqnarray*}
R_{20,1}(g,g_0,\bar{Q},\bar{Q}_0)&=& E_{P_0} \left( \frac{g_0(1\mid W)}{g(1\mid W)}-1\right) I(B(W)<y)(\bar{Q}_0-\bar{Q})(1,W)\\
&&-E_{P_0}\left ( \frac{g_0(0\mid W)}{g(0\mid W)}-1\right) I(B(W)<y)(\bar{Q}_0-\bar{Q})(0,W).
\end{eqnarray*}
So it remains to analyze:
\[
-R_{20,2}(B,B_0)\equiv \int_{-1}^y P_0\{I(B>x)-I(B_0>x)\}dx-P_0I(B<y)(B-B_0),\]
and note that $R_{20}(P,P_0)=R_{20,1}(P,P_0)+R_{20,2}(P,P_0)$.
By Fubini's theorem, we can write this as:
\[
\begin{array}{l}
R_{20,2}(B,B_0)=E_{P_0} \int_{-1}^y I(B(W)>x)dx -E_{P_0}\int_{-1}^y I(B_0(W)>x) dx-P_0I(B<y)(B-B_0)\\
=E_{P_0} (\min(y,B(W))+1)-E_{P_0}(\min(y,B_0(W))+1) -P_0I(B<y)(B-B_0)\\
=E_{P_0}(\min(y,B(W))-\min(y,B_0(W))) -P_0I(B<y)(B-B_0)\\
=E_{P_0}I(B<y,B_0<y)(B-B_0)+E_{P_0}I(B<y,B_0>y)(B-y)+E_{P_0}I(B>y,B_0<y)(y-B_0)\\
-P_0I(B<y)(B-B_0)\\
=P_0\{I(B<y,B_0<y)-I(B<y)\}(B-B_0)+P_0I(B<y,B_0>y)(B-y)\\
+P_0I(B>y,B_0<y)(y-B_0)\\
=P_0 I(B<y,B_0>y)(B_0-B)+P_0I(B<y,B_0>y)(B-y)\\
+P_0I(B>y,B_0<y)(y-B_0)\\
=P_0 I(B<y,B_0>y)(B_0-y)+P_0I(B>y,B_0<y)(y-B_0)
\end{array}
\]
where we used that $I(B<y,B_0<y)-I(B<y)=I(B<y)(I(B_0<y)-1)=-I(B<y,B_0>y)$.
Note that both terms are positive. 
Note that
\begin{eqnarray*}
0\leq P_0I(B<y,B_0>y)(B_0-y) &\leq& P_0 I(B<y,B_0>y)(B_0-B)\\
0\leq P_0I(B>y,B_0<y)(y-B_0) &\leq&P_0I(B>y,B_0<y)(B-B_0)\\
\end{eqnarray*}
So we have that $R_{20,2}(B,B_0)\geq 0$ and 
\begin{eqnarray*}
R_{20,2}(B,B_0)&\leq & P_0I(B<y,B_0>y)(B_0-B)
+P_0I(B>y,B_0<y)(B-B_0).
\end{eqnarray*}

This is indeed a higher order remainder. For example, using Cauchy-Schwarz inequality, we could bound $R_{20,2}$ as follows:
\begin{eqnarray*}
\mid R_{20,2}(B,B_0)\mid &\leq& \pl B-B_0\pl_{P_0}\pl I(B<y,B_0>y)\pl_{P_0}\\
&&+\pl B-B_0\pl_{P_0}\pl I(B>y,B_0<y)\pl_{P_0}\\
&=& \pl B-B_0\pl_{P_0}\left\{\sqrt{P_0(B<y,B_0>y)}+\sqrt{P(B>y,B_0<y)}\right\}.
\end{eqnarray*}
Using this Cauchy-Schwarz bounding, the remainder appears to behave as a $(B-B_0)^{3/2}$, intuitively speaking. 
However, one can also bound it as 
\[
R_{20,2}(B,B_0)\leq  \pl B_0-B\pl_{\infty}\left\{P_0(B<y,B_0>y)+P_0(B>y,B_0<y)\right\},\]
which  is a second order difference. 
To further understand this remainder, we note that
\begin{eqnarray*}
P_0(B<y,B_0>y)&=&E_0 I(B(W)<y,B_0(W)>y)\\
&=& E_0 I(B(W)<y,B_0(W)>y)I(B_0(W)-B(W)>B_0(W)-y)\\
&\leq &E_0I(B(W)<y,B_0(W)>y)I(\pl B_0-B\pl_{\infty}>B_0(W)-y)\\
&=& P_0(y<B_0(W)<y+\pl B_0-B\pl_{\infty},B(W)<y)\\
&\leq & F_{B_0}(y+\pl B_0-B\pl_{\infty})-F_{B_0}(y),\end{eqnarray*}
where $F_{B_0}(y)=P_0(B_0\leq y)$ is the cdc  of $B_0$.
If we assume that $F_{B_0}$ is Lipsnitz-continuous, then this can be bounded by 
a constant times $\pl B_0-B\pl_{\infty}$.
Similarly,
\begin{eqnarray*}
P_0(B>y,B_0<y)&=&E_0 I(B(W)>y,B_0(W)<y)\\
&=& E_0 I(B(W)>y,B_0(W)<y)I(y-B_0(W)<B(W)-B_0(W)) \\
&\leq & E_0I(B(W)>y,B_0(W)<y) I(y-B_0(W)<\pl B-B_0\pl_{\infty})\\
&=& P_0(y-\pl B-B_0\pl_{\infty}<B_0(W)<y,B(W)>y)\\
&\leq & F_{B_0}(y)-F_{B_0}(y-\pl B_0-B\pl_{\infty}).\end{eqnarray*}


This proves the following theorem.

\begin{theorem}
Let
\[
R_{20}(P,P_0)=P_0D^*(P)+\Psi(P)-\Psi(P_0).\]
We have
\[
R_{20}(P,P_0)=R_{20,1}(g,g_0,\bar{Q},\bar{Q}_0)+R_{20,2}(B,B_0),\]
where
\begin{eqnarray*}
R_{20,1}(g,g_0,\bar{Q},\bar{Q}_0)&=& E_{P_0} \left( \frac{g_0(1\mid W)}{g(1\mid W)}-1\right) I(B(W)<y)(\bar{Q}_0-\bar{Q})(1,W)\\
&&-E_{P_0}\left ( \frac{g_0(0\mid W)}{g(0\mid W)}-1\right) I(B(W)<y)(\bar{Q}_0-\bar{Q})(0,W),
\end{eqnarray*}
and
\[
0\leq R_{20,2}(B,B_0)=P_0I(B<y,B_0>y)(B_0-B)
+P_0I(B>y,B_0<y)(B-B_0).
\]
We can bound the latter term in the following two ways.
\begin{eqnarray*}
R_{20,2}(B,B_0)&\leq & \pl B-B_0\pl_{P_0}\left\{\sqrt{P_0(B<y,B_0>y)}+\sqrt{P(B>y,B_0<y)}\right\}\\
R_{20,2}(B,B_0)&\leq & \pl B_0-B\pl_{\infty}\left\{P_0(B<y,B_0>y)+P_0(B>y,B_0<y)\right\},
\end{eqnarray*}
and 
\begin{eqnarray*}
P_0(B<y,B_0>y)&\leq& F_{B_0}(y+\pl B_0-B\pl_{\infty})-F_{B_0}(y)\\
P_0(B>y,B_0<y)&\leq & F_{B_0}(y)-F_{B_0}(y-\pl B_0-B\pl_{\infty}),
\end{eqnarray*}
where $F_{B_0}$ is the cumulative distribution function of $B_0$.
\end{theorem}

\end{document}
























